# GitHub: https://github.com/naotaka1128/llm_app_codes/chapter_010/main_feedback_new.py

import streamlit as st
import uuid  #  thread_id ìƒì„±ìš©

# ============================================================
#  LangChain 1.0.0+ ì‹ ê·œ create_agent ì‚¬ìš©
# ============================================================
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware
from langgraph.checkpoint.memory import InMemorySaver

# models
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI

# custom tools
from tools.fetch_qa_content import fetch_qa_content
from tools.fetch_stores_by_prefecture import fetch_stores_by_prefecture

# cache / feedback
from src.cache import Cache
from src.feedback import add_feedback

#  StreamlitLanggraphHandler ì‚¬ìš© (ê¸°ì¡´ StreamlitCallbackHandler ëŒ€ì²´)
from youngjin_langchain_tools import StreamlitLanggraphHandler

# LangSmith trace
from langsmith import traceable

###### dotenvë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°ëŠ” ì‚­ì œí•´ì£¼ì„¸ìš” ######
try:
    from dotenv import load_dotenv

    load_dotenv()
except ImportError:
    import warnings

    warnings.warn(
        "dotenv not found. Please make sure to set your environment variables manually.",
        ImportWarning,
    )
################################################


@st.cache_data
def load_system_prompt(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read()


# ============================================================
# Streamlit UI Functions
# ============================================================
def init_page():
    st.set_page_config(page_title="ê³ ê° ì§€ì›", page_icon="ğŸ»")
    st.header("ê³ ê° ì§€ì›ğŸ»")
    st.sidebar.title("Options")


def init_messages():
    clear_button = st.sidebar.button("Clear Conversation", key="clear")
    if clear_button or "messages" not in st.session_state:
        welcome_message = (
            "ì˜ì§„ëª¨ë°”ì¼ ê³ ê°ì§€ì›ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš” ğŸ»"
        )
        st.session_state.messages = [{"role": "assistant", "content": welcome_message}]
        #  ConversationBufferWindowMemory ëŒ€ì‹  InMemorySaver + thread_id ì‚¬ìš©
        st.session_state["checkpointer"] = InMemorySaver()
        st.session_state["thread_id"] = str(uuid.uuid4())

    st.session_state["first_question"] = len(st.session_state.messages) == 1


def select_model(temperature=0):
    models = ("GPT-5 mini", "GPT-5.2", "Claude Sonnet 4.5", "Gemini 2.5 Flash")
    model = st.sidebar.radio("ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ:", models)
    if model == "GPT-5 mini":
        return ChatOpenAI(temperature=temperature, model="gpt-5-mini")
    elif model == "GPT-5.2":
        return ChatOpenAI(temperature=temperature, model="gpt-5.2")
    elif model == "Claude Sonnet 4.5":
        return ChatAnthropic(
            temperature=temperature, model="claude-sonnet-4-5-20250929"
        )
    elif model == "Gemini 2.5 Flash":
        return ChatGoogleGenerativeAI(temperature=temperature, model="gemini-2.5-flash")


# ============================================================
#  ì—ì´ì „íŠ¸ ìƒì„± ë°©ì‹ ë³€ê²½
# (create_tool_calling_agent + AgentExecutor â†’ create_agent)
# ============================================================
def create_customer_support_agent():
    tools = [fetch_qa_content, fetch_stores_by_prefecture]
    custom_system_prompt = load_system_prompt("./prompt/system_prompt.txt")
    llm = select_model()

    #  SummarizationMiddleware ì¶”ê°€
    summarization_middleware = SummarizationMiddleware(
        model=llm,
        max_tokens_before_summary=8000,
        messages_to_keep=10,
    )

    #  create_agent ì‚¬ìš© (system_prompt ì§ì ‘ ì „ë‹¬, checkpointer ì‚¬ìš©)
    agent = create_agent(
        model=llm,
        tools=tools,
        system_prompt=custom_system_prompt,
        checkpointer=st.session_state["checkpointer"],
        middleware=[summarization_middleware],
        debug=True
    )

    return agent


# ============================================================
#  run_agent í•¨ìˆ˜ ë³€ê²½ - StreamlitLanggraphHandler ì‚¬ìš©
# ============================================================
@traceable  # LangSmith íŠ¸ë ˆì´ìŠ¤
def run_agent(agent, user_input, handler, thread_id):
    """ì—ì´ì „íŠ¸ ì‹¤í–‰ ë° ì‘ë‹µ ë°˜í™˜ (LangSmith traceable)"""
    response = handler.invoke(
        agent=agent,
        input={"messages": [{"role": "user", "content": user_input}]},
        config={"configurable": {"thread_id": thread_id}}
    )
    return response


# ============================================================
# Main Function - StreamlitLanggraphHandler ì‚¬ìš©
# ============================================================
def main():
    init_page()
    init_messages()

    if "run_id" not in st.session_state:
        st.session_state["run_id"] = None

    customer_support_agent = create_customer_support_agent()
    cache = Cache()

    #  ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ ë°©ì‹ ë³€ê²½
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    if prompt := st.chat_input(placeholder="ë²•ì¸ ëª…ì˜ë¡œ ê³„ì•½ì´ ê°€ëŠ¥í•œê°€ìš”?"):
        st.chat_message("user").write(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        # ì²« ë²ˆì§¸ ì§ˆë¬¸ì¸ ê²½ìš° ìºì‹œ í™•ì¸
        if st.session_state["first_question"]:
            if cache_content := cache.search(query=prompt):
                with st.chat_message("assistant"):
                    st.write(f"(cache) {cache_content}")
                st.session_state.messages.append(
                    {"role": "assistant", "content": cache_content}
                )
                st.stop()

        with st.chat_message("assistant"):
            #  StreamlitLanggraphHandler ì‚¬ìš© (ê¸°ì¡´ StreamlitCallbackHandler ëŒ€ì²´)
            handler = StreamlitLanggraphHandler(
                container=st.container(),
                expand_new_thoughts=True,
                max_thought_containers=4,
            )

            #  ì—ì´ì „íŠ¸ í˜¸ì¶œ ë°©ì‹ ë³€ê²½
            response = run_agent(
                customer_support_agent,
                prompt,
                handler,
                st.session_state["thread_id"]
            )

            # [NOTE] run_idëŠ” LangSmith traceable ë°ì½”ë ˆì´í„°ì—ì„œ ìë™ ìƒì„±ë¨
            # í•„ìš”ì‹œ handler ë˜ëŠ” langsmith í´ë¼ì´ì–¸íŠ¸ì—ì„œ run_id ê°€ì ¸ì˜¤ê¸°
            if hasattr(handler, 'run_id'):
                st.session_state["run_id"] = str(handler.run_id)
                print("ğŸ”¥ RUN ID GENERATED:", st.session_state["run_id"])

            # ì‘ë‹µ ì €ì¥
            if response:
                st.session_state.messages.append({"role": "assistant", "content": response})

        # ì²« ë²ˆì§¸ ì§ˆë¬¸ì´ë©´ ìºì‹œì— ì €ì¥
        if st.session_state["first_question"] and response:
            cache.save(prompt, response)

    # LangSmith feedback ë²„íŠ¼ ìœ ì§€
    add_feedback()


if __name__ == "__main__":
    main()
