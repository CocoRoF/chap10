# GitHub: https://github.com/naotaka1128/llm_app_codes/chapter_010/main_cache_new.py

import streamlit as st
import uuid  #  thread_id ìƒì„±ìš©

# ============================================================
#  LangChain 1.0.0+ ì‹ ê·œ create_agent ì‚¬ìš©
# ============================================================
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware
from langgraph.checkpoint.memory import InMemorySaver

# models
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI

# custom tools
from tools.fetch_qa_content import fetch_qa_content
from tools.fetch_stores_by_prefecture import fetch_stores_by_prefecture
from src.cache import Cache

#  StreamlitLanggraphHandler ì‚¬ìš© (ê¸°ì¡´ StreamlitCallbackHandler ëŒ€ì²´)
from youngjin_langchain_tools import StreamlitLanggraphHandler

###### dotenvë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°ëŠ” ì‚­ì œí•´ì£¼ì„¸ìš” ######
try:
    from dotenv import load_dotenv

    load_dotenv()
except ImportError:
    import warnings

    warnings.warn(
        "dotenv not found. Please make sure to set your environment variables manually.",
        ImportWarning,
    )
################################################


@st.cache_data  # ìºì‹œë¥¼ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½
def load_system_prompt(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read()


# ============================================================
# Streamlit UI Functions
# ============================================================
def init_page():
    st.set_page_config(page_title="ê³ ê° ì§€ì›", page_icon="ğŸ»")
    st.header("ê³ ê° ì§€ì›ğŸ»")
    st.sidebar.title("Options")


def init_messages():
    clear_button = st.sidebar.button("Clear Conversation", key="clear")
    if clear_button or "messages" not in st.session_state:
        welcome_message = (
            "ì˜ì§„ëª¨ë°”ì¼ ê³ ê°ì§€ì›ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš” ğŸ»"
        )
        st.session_state.messages = [{"role": "assistant", "content": welcome_message}]
        #  ConversationBufferWindowMemory ëŒ€ì‹  InMemorySaver + thread_id ì‚¬ìš©
        st.session_state["checkpointer"] = InMemorySaver()
        st.session_state["thread_id"] = str(uuid.uuid4())

    if len(st.session_state.messages) == 1:  # í™˜ì˜ ë©”ì‹œì§€ë¿ì¸ ê²½ìš°
        st.session_state["first_question"] = True
    else:
        st.session_state["first_question"] = False


def select_model(temperature=0):
    models = ("GPT-5 mini", "GPT-5.2", "Claude Sonnet 4.5", "Gemini 2.5 Flash")
    model = st.sidebar.radio("ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ:", models)
    if model == "GPT-5 mini":
        return ChatOpenAI(temperature=temperature, model="gpt-5-mini")
    elif model == "GPT-5.2":
        return ChatOpenAI(temperature=temperature, model="gpt-5.2")
    elif model == "Claude Sonnet 4.5":
        return ChatAnthropic(
            temperature=temperature, model="claude-sonnet-4-5-20250929"
        )
    elif model == "Gemini 2.5 Flash":
        return ChatGoogleGenerativeAI(temperature=temperature, model="gemini-2.5-flash")


# ============================================================
#  ì—ì´ì „íŠ¸ ìƒì„± ë°©ì‹ ë³€ê²½
# (create_tool_calling_agent + AgentExecutor â†’ create_agent)
# ============================================================
def create_customer_support_agent():
    tools = [fetch_qa_content, fetch_stores_by_prefecture]
    # ìºì‹œë¥¼ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½
    custom_system_prompt = load_system_prompt("./prompt/system_prompt.txt")
    llm = select_model()

    #  SummarizationMiddleware ì¶”ê°€
    summarization_middleware = SummarizationMiddleware(
        model=llm,
        max_tokens_before_summary=8000,
        messages_to_keep=10,
    )

    #  create_agent ì‚¬ìš© (system_prompt ì§ì ‘ ì „ë‹¬, checkpointer ì‚¬ìš©)
    agent = create_agent(
        model=llm,
        tools=tools,
        system_prompt=custom_system_prompt,
        checkpointer=st.session_state["checkpointer"],
        middleware=[summarization_middleware],
        debug=True
    )

    return agent


# ============================================================
# Main Function - StreamlitLanggraphHandler ì‚¬ìš©
# ============================================================
def main():
    init_page()
    init_messages()
    customer_support_agent = create_customer_support_agent()

    # ìºì‹œ ì´ˆê¸°í™”
    cache = Cache()

    #  ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ ë°©ì‹ ë³€ê²½
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    if prompt := st.chat_input(placeholder="ë²•ì¸ ëª…ì˜ë¡œ ê³„ì•½ì´ ê°€ëŠ¥í•œê°€ìš”?"):
        st.chat_message("user").write(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        # ì²˜ìŒ ì§ˆë¬¸ì¸ ê²½ìš° ìºì‹œë¥¼ í™•ì¸
        if st.session_state["first_question"]:
            if cache_content := cache.search(query=prompt):
                st.chat_message("assistant").write(f"(cache) {cache_content}")
                st.session_state.messages.append(
                    {"role": "assistant", "content": cache_content}
                )
                st.stop()  # ìºì‹œ ë‚´ìš©ì„ ì¶œë ¥í•œ ê²½ìš° ì‹¤í–‰ ì¢…ë£Œ

        with st.chat_message("assistant"):
            #  StreamlitLanggraphHandler ì‚¬ìš© (ê¸°ì¡´ StreamlitCallbackHandler ëŒ€ì²´)
            handler = StreamlitLanggraphHandler(
                container=st.container(),
                expand_new_thoughts=True,
                max_thought_containers=4,
            )

            #  ì—ì´ì „íŠ¸ í˜¸ì¶œ ë°©ì‹ ë³€ê²½
            response = handler.invoke(
                agent=customer_support_agent,
                input={"messages": [{"role": "user", "content": prompt}]},
                config={"configurable": {"thread_id": st.session_state["thread_id"]}}
            )

            # ì‘ë‹µ ì €ì¥
            if response:
                st.session_state.messages.append({"role": "assistant", "content": response})

        # ì²˜ìŒ ì§ˆë¬¸ì¸ ê²½ìš° ìºì‹œì— ì €ì¥
        if st.session_state["first_question"] and response:
            cache.save(prompt, response)


if __name__ == "__main__":
    main()
